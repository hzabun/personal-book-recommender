{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add missing information to books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_csv(\"../data/processed/books.csv\")\n",
    "ratings_df = pd.read_csv(\"../data/processed/ratings.csv\")\n",
    "user_book_ratings_df = pd.read_csv(\"../data/processed/user_book_ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing columsn to user_book_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting rows with missing values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_book_ratings_df_missing = user_book_ratings_df[user_book_ratings_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the Open Library API to send requrests with ISBN to get genre, title, authors etc.\n",
    "\n",
    "The API allowed multiple ISBNs to be requested, but 300 seems to be the upper limit. So 300 ISBNs are requested each, their information extraced and then added to our books dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The script below is way more complex than it should be and on a normal day I wouldn't touch it with a 10 foot pole. But as this processing is a one-time thing I didn't invest much into making it more pythonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 300\n",
    "\n",
    "for i in range(0, len(user_book_ratings_df_missing), chunk_size):\n",
    "    isbns = user_book_ratings_df_missing['isbn'].iloc[i:i+chunk_size]\n",
    "    isbns_formatted = '|'.join(isbns)\n",
    "    url = f\"http://openlibrary.org/api/volumes/brief/json/{isbns_formatted}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    for book_info in response.json().values():\n",
    "        for record in book_info['records'].values():\n",
    "            data = record['data']\n",
    "            genre_dicts = data.get('subjects', None)\n",
    "\n",
    "            new_data = {'book_title': data.get('title', None),\n",
    "                        'book_author': data.get('authors', None),\n",
    "                        'publication_year': data.get('publish_date', None),\n",
    "                        'publisher': data.get('publishers', None)}\n",
    "\n",
    "            row_index = user_book_ratings_df_missing[user_book_ratings_df_missing['isbn'] == record['isbns'][0]].index[0]\n",
    "            user_book_ratings_df_missing.loc[row_index, new_data.keys()] = new_data.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add genre to books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a \"genre\" column with empty lists to populate later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['genre'] = np.empty((len(books_df), 0)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, this time only adding genres to the books_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 300\n",
    "\n",
    "for i in range(0, len(books_df), chunk_size):\n",
    "    isbns = books_df['isbn'].iloc[i:i+chunk_size]\n",
    "    isbns_formatted = '|'.join(isbns)\n",
    "    url = f\"http://openlibrary.org/api/volumes/brief/json/{isbns_formatted}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    for book_info in response.json().values():\n",
    "        for record in book_info['records'].values():\n",
    "            data = record['data']\n",
    "            genre_dicts = data.get('subjects', None)\n",
    "\n",
    "            if genre_dicts is not None:\n",
    "                genre_list = [genre['name'] for genre in genre_dicts]\n",
    "                row_index = books_df[books_df['isbn'] == record['isbns'][0]].index[0]\n",
    "                books_df.loc[row_index, 'genre'] = genre_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's...taking ages.\n",
    "\n",
    "Connection to the API drops all the time without even reaching the 300 chunk size. The *user_book_rating_df_missing* has 109076 rows and *books_df* has 270947 rows. It would take literally ages to finish this whole processing.\n",
    "\n",
    "So I shifted my approach to this project to leverage my expertise with LLMs.\n",
    "\n",
    "The goal now is to utilize LLMs to later take the output of the recommender systems and expand information like genre on it with its own knowledge (or a potential dyanmic web scraping)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
